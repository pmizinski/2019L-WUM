---
title: "K-średnie oraz K-medoidy"
author: "Szymon Maksymiuk"
date: "03.06.2019"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: true
    theme: spacelab
---

```{r message=FALSE, warning=FALSE}
library(patchwork)
library(factoextra)
library(cluster)
data <- read.csv("Sales_Transactions_Dataset_Weekly.csv")
data <- data[,-(1:53)]
```

# Wstęp

W poniższej pracy przyjrzę się dwóm metodom klasteryzacji, k-średnich oraz k-medoidów. Zbiorem danych na którym dokonam obliczeń będą dane sprzedaży w pewnym sklepie, każdy wiersz zawiera znormalizowaną liczność danego produktu zakupioonego w danym tyogdniu. Oryginalne dane znajdują się [tutaj](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly). W pracy na początek przedstawię optymalną liczbę klastrów dla każdej z tych metod, a następnie porównam obie klasteryzacje.

# Optymalna liczba klastrów

U wyznaczenia użyjemy metody łokcia. Oraz zwykłego wyresu w base. Dla k-średnich użyjemy statystyki WSS (within-clusters sum of squares), zaś dla k-medoidów domyślnej funkcji `objective` (więcej ?pam.object)

## k-średnie

```{r}
set.seed(123)
k.max <- 12
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```
Widzimy, że w tym wypadku dla k-średnich najlepszym wyjściem są 3 klastry. Dalsze zwiększanie liczby klastróW nie ma już sensu.

## k-medoidy

```{r}
set.seed(123)
k.max <- 12
objective <- sapply(1:k.max, 
              function(k){pam(data, k)$objective})
plot(1:k.max, objective[1,],
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Objective function")
```
W przypadku k-medoidów najbardziej optymalna wydaje się opcja 4 klastrów. Utrudni to nam nieco analize ale nie ma co się bać.

# Porównanie klastrów

```{r}
kmeans_clust <- kmeans(data, 3, nstart=50,iter.max = 15 )
kmedoids_clust <- pam(data, 4)
```

Na początek przedstawmy wizualizację naszego klastrowania

```{r}
plot_kmeans <- fviz_cluster(kmeans_clust, geom = "point",  data = data, shape = 1, show.clust.cent=TRUE) +
  ggtitle("k-średnie") + theme(legend.position = "none") + theme_bw()
plot_kmedoids <- fviz_cluster(kmedoids_clust, geom = "point",  data = data, shape = 1, show.clust.cent=TRUE) +
  ggtitle("k-medoidy") + theme(legend.position = "none") + theme_bw()
plot_kmeans + plot_kmedoids
```

Jak widzimy wizualizacja naszego podziału nie jest porywająca. Zbiór jest trudno zeparowalny naszymi metodami. Bezcelowe jest też liczenie indeksów podobieństwa pomiędzy tymi dwoma klastrowaniami z racji na różnicę w wybranej liczbie k. Skupmy się więc na porównaniu centrów. Zrobimy to w sposób następujący. Centra algorytmu kmeans zrzutujemy na przestrzeń 4wymiarową a następnie policzymy odległość euklidesową. 

```{r}
distance <- NULL
for (i in 1:ncol(data)) {
  distance <- c(distance, dist(rbind(
    c(kmeans_clust$centers[,i], 0),
    kmedoids_clust$medoids[,i]
  )))
}
hist(distance[distance < 2], main = "Rozkład odległości", xlab = "Odległość")
```

Na histogrami widzimy rozkład odległości pomiędzy centrami a medoidami. Usunęliśmy jedną odległość odstającą, tak aby histogram był bardziej widoczny. W ogólnosci odległości te nie są małe, tymbardziej, że 51 na 54 kolumn zbioru zostały normalizowane.

# Szybkość

Algorytm `kmeans` zbieżby jest już po 3 iteracjach. Niestety `pam` nie udsotępnia ilości iteracji. Przedstawię jednak żłożoność obliczeniową obu algorytmów dzięki funkcji `microbenchmark`.

```{r, warning=FALSE, message=FALSE}
benchmark <- microbenchmark::microbenchmark(
  
kmeans = kmeans(data, 3, nstart=50,iter.max = 15),
pam = pam(data, 4),
times = 100
  
  
)
autoplot(benchmark)
```

Na podstawie powyżego wykresu stwierdzamy, że algorytm `kmeans` jest nieco szybszy.